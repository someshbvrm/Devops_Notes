
---

# ⚡ Kubernetes Scheduling & Availability

---

## 🖥️ Node Maintenance

### 🔹 Drain a Node

Safely evict all pods before maintenance (e.g., kernel upgrades, reboot).

```bash
kubectl drain <node-name> --ignore-daemonsets
```

* ✅ Marks node **unschedulable**
* ✅ Respects **PodDisruptionBudgets (PDBs)**
* ❌ DaemonSets are not evicted (unless forced)

👉 After maintenance, make the node schedulable again:

```bash
kubectl uncordon <node-name>
```

---

## 🏷️ Labels for Scheduling

Attach labels to nodes:

```bash
kubectl label nodes <node-name> disktype=ssd
kubectl label nodes <node-name> project=itcm
kubectl label nodes <node-name> project=boa
```

Check labels:

```bash
kubectl get nodes --show-labels
```

---

## 🎯 Scheduling Rules

### 🔹 NodeSelector (Simple Matching)

Schedule Pod on a node with a matching label.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
  nodeSelector:
    disktype: ssd
```

---

### 🔹 NodeName (Direct Binding)

Force Pod to a specific node by name.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  nodeName: foo-node   # Specific node name
  containers:
  - name: nginx
    image: nginx
```

---

### 🔹 Node Affinity (Flexible Rules)

* `requiredDuringSchedulingIgnoredDuringExecution` → Hard rule
* `preferredDuringSchedulingIgnoredDuringExecution` → Soft rule

👉 Example:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: topology.kubernetes.io/zone
            operator: In
            values:
            - antarctica-east1
            - antarctica-west1
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: pause
    image: registry.k8s.io/pause:3.8
```

---

### ⚖️ Node Affinity with Weights

Nodes are scored by weights → higher scores are prioritized.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-affinity-preferred-weight
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/os
            operator: In
            values:
            - linux
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: label-1
            operator: In
            values:
            - key-1
      - weight: 50
        preference:
          matchExpressions:
          - key: label-2
            operator: In
            values:
            - key-2
  containers:
  - name: pause
    image: registry.k8s.io/pause:3.8
```

---

## 🤝 Pod Affinity & Anti-Affinity

* **Pod Affinity** → Pods like to co-locate with others
* **Pod Anti-Affinity** → Pods avoid co-locating

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: topology.kubernetes.io/zone
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: topology.kubernetes.io/zone
  containers:
  - name: pause
    image: registry.k8s.io/pause:3.8
```

---

## 🚫 Taints & Tolerations

* **Taints (Node)** → Repel pods 🚷
* **Tolerations (Pod)** → Allow pods to run despite taints

### Taint Node

```bash
kubectl taint nodes node1 reserved=itcm:NoSchedule
```

### Pod with Toleration

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
  tolerations:
  - key: "reserved"
    operator: "Equal"
    value: "itcm"
    effect: "NoSchedule"
```

---

## 🛡️ PodDisruptionBudgets (PDB)

Ensure availability during **voluntary disruptions** (e.g., `drain`, upgrades).

### Example with `minAvailable`

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: zk-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: zookeeper
```

### Example with `maxUnavailable`

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: zk-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: zookeeper
```

Check PDBs:

```bash
kubectl get poddisruptionbudgets
```

---

✨ **Summary**

* ✅ **NodeSelector / Node Affinity** → Where Pods *should* run
* ✅ **Taints & Tolerations** → Where Pods *shouldn’t* run
* ✅ **Pod Affinity/Anti-Affinity** → Pods scheduling based on other Pods
* ✅ **PDB** → Keep apps available during maintenance

---


# 📊 Kubernetes Scheduling & Availability – Comparison Table

| 🔧 Feature                       | 📍 Scope    | ✅ What it Does                                                  | ⚠️ Limitation                                     | 📝 Example Usage                                 |
| -------------------------------- | ----------- | --------------------------------------------------------------- | ------------------------------------------------- | ------------------------------------------------ |
| **NodeSelector** 🏷️             | Node labels | Schedule pods on nodes matching exact labels                    | Very basic, only supports equality-based matching | `nodeSelector: disktype=ssd`                     |
| **Node Affinity** 🎯             | Node labels | Advanced rules (In, NotIn, Exists, etc.), soft/hard constraints | Pod won’t move if node labels change              | Schedule only on nodes in `zone=us-east1`        |
| **Pod Affinity** 🤝              | Other Pods  | Schedule pods *together* with pods that have specific labels    | Risk of overloading certain nodes                 | Place frontend pod with backend pod in same zone |
| **Pod Anti-Affinity** 🙅         | Other Pods  | Ensure pods *don’t* run with other specific pods                | Can reduce scheduling flexibility                 | Spread replicas across zones/nodes               |
| **Taints** 🚫                    | Node-level  | Repel pods from nodes unless tolerated                          | Alone doesn’t guarantee scheduling                | Reserve nodes for system workloads               |
| **Tolerations** 🛡️              | Pod-level   | Allow pods to tolerate tainted nodes                            | Doesn’t *force* scheduling                        | Critical pods run on tainted infra nodes         |
| **PodDisruptionBudget (PDB)** 🛑 | Workloads   | Ensures min availability during voluntary disruptions           | Doesn’t protect from node crashes                 | Keep 4/5 replicas running during upgrade         |
| **kubectl drain** 🧹             | Node        | Safely evict pods before maintenance                            | Evicts only voluntary workloads (respects PDB)    | `kubectl drain node1 --ignore-daemonsets`        |

---

⚡ **Quick Rules to Remember**:

* Use **NodeSelector/NodeAffinity** → control **where pods run**.
* Use **Taints & Tolerations** → control **where pods don’t run**.
* Use **Pod Affinity/Anti-Affinity** → control **pod-to-pod co-location**.
* Use **PDB** → control **availability during maintenance**.

---
