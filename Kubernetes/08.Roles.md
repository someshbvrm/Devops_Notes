
---

# ğŸ” Kubernetes RBAC (Role-Based Access Control)

ğŸ“Œ **RBAC** regulates access to resources based on **roles** assigned to users/groups.

* Permissions are **additive only** (no "deny" rules).
* Used to control **who can do what** in Kubernetes.

---

## ğŸŸ¢ Role vs ClusterRole

* **Role**

  * Namespaced ğŸ”–
  * Defines permissions **only within a namespace**

* **ClusterRole**

  * Cluster-scoped ğŸŒ
  * Can be used for:

    * Cluster-wide resources (e.g., `nodes`)
    * Namespaced resources across **all namespaces**
    * Non-resource endpoints (e.g., `/healthz`)

---

### ğŸ“ Example â€“ Role

ğŸ“‚ `access/simple-role.yaml`

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]   # "" = core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
```

---

### ğŸ“ Example â€“ ClusterRole

ğŸ“‚ `access/simple-clusterrole.yaml`

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: secret-reader
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
```

---

## ğŸ”µ RoleBinding vs ClusterRoleBinding

* **RoleBinding** â†’ Grants Role (or ClusterRole) permissions **within a namespace**
* **ClusterRoleBinding** â†’ Grants ClusterRole permissions **across the entire cluster**

---

### ğŸ“ Example â€“ RoleBinding

ğŸ“‚ `access/simple-rolebinding-with-role.yaml`

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: jane   # case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role         # Role or ClusterRole
  name: pod-reader   # must match Role/ClusterRole name
  apiGroup: rbac.authorization.k8s.io
```

---

### ğŸ“ Example â€“ ClusterRoleBinding

ğŸ“‚ `access/simple-clusterrolebinding.yaml`

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-secrets-global
subjects:
- kind: Group
  name: manager   # case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io
```

---

## â˜ï¸ RBAC in EKS (IAM vs RBAC)

* **IAM Roles** â†’ AWS-native, used for **authentication** (who you are).
* **Kubernetes RBAC** â†’ Used for **authorization** (what you can do).

ğŸ‘‰ In EKS:

* IAM controls **cluster access** (via `aws-auth` ConfigMap).
* RBAC controls **in-cluster permissions**.

---

# ğŸ› ï¸ Kubernetes Troubleshooting

When things break in K8s, start **from Pods â†’ Services â†’ Ingress â†’ Networking**.

---

## ğŸŸ¢ Pods & Nodes

```bash
kubectl get pods -n <namespace>
kubectl describe pod <pod-name> -n <namespace>
kubectl logs <pod-name> -c <container-name>
kubectl exec <pod-name> -c <container-name> -- <command>
kubectl get events -n <namespace>
```

Check node state:

```bash
kubectl get nodes
kubectl describe node <node-name>
```

---

## ğŸ”µ Services

Check if Service is exposing correctly:

```bash
kubectl get svc <service-name>
kubectl describe svc <service-name>
```

ğŸ”¹ Things to check:

* â“ Does the Service have **endpoints**?

  ```bash
  kubectl get endpoints <service-name>
  ```
* ğŸ”„ Is **kube-proxy** running on nodes?

  ```bash
  ps auxw | grep kube-proxy
  ```
* ğŸ” Any **NetworkPolicies** blocking traffic?

---

## ğŸŸ  Ingress

* âœ… Is Ingress Controller running?
* ğŸ” Check logs of Ingress Controller:

  ```bash
  kubectl logs <ingress-controller-pod> -n <namespace>
  ```
* ğŸŒ Verify DNS and ingress rules match service names.

---

âš¡ **Pro-Tip**: If backend Pods are working fine, the issue is usually in **Service, Ingress, or Network Policies**.

---
